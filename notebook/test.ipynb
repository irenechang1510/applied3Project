{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from turbo.gp import train_gp  # Ensure this module is available\n",
    "from turbo import Turbo1  # Ensure this module is available\n",
    "from turbo.utils import from_unit_cube, latin_hypercube, to_unit_cube\n",
    "\n",
    "# Import BoTorch components\n",
    "from botorch.acquisition import AcquisitionFunction\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "\n",
    "class TurboM(Turbo1):\n",
    "    \"\"\"The TuRBO-M algorithm with a customizable acquisition function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        The objective function to minimize.\n",
    "    lb : numpy.ndarray\n",
    "        Lower bounds of the variables, shape (d,).\n",
    "    ub : numpy.ndarray\n",
    "        Upper bounds of the variables, shape (d,).\n",
    "    n_init : int\n",
    "        Number of initial points *for each trust region* (2 * dim is recommended).\n",
    "    max_evals : int\n",
    "        Total evaluation budget.\n",
    "    n_trust_regions : int\n",
    "        Number of trust regions.\n",
    "    batch_size : int, optional\n",
    "        Number of points in each batch (default is 1).\n",
    "    verbose : bool, optional\n",
    "        If True, prints information about the optimization progress (default is True).\n",
    "    use_ard : bool, optional\n",
    "        If True, uses Automatic Relevance Determination for the GP kernel (default is True).\n",
    "    max_cholesky_size : int, optional\n",
    "        Largest number of training points where Cholesky decomposition is used (default is 2000).\n",
    "    n_training_steps : int, optional\n",
    "        Number of training steps for learning the GP hyperparameters (default is 50).\n",
    "    min_cuda : int, optional\n",
    "        Minimum number of points to use CUDA for GP fitting (default is 1024).\n",
    "    device : str, optional\n",
    "        Device to use for GP fitting (\"cpu\" or \"cuda\") (default is \"cpu\").\n",
    "    dtype : str, optional\n",
    "        Data type to use for GP fitting (\"float32\" or \"float64\") (default is \"float64\").\n",
    "    utility_function_class : class, optional\n",
    "        Custom utility function class for the acquisition function (default is None).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    turbo_m = TurboM(\n",
    "        f=f,\n",
    "        lb=lb,\n",
    "        ub=ub,\n",
    "        n_init=10,\n",
    "        max_evals=200,\n",
    "        n_trust_regions=5,\n",
    "        utility_function_class=ExpectedImprovementCustom,\n",
    "    )\n",
    "    turbo_m.optimize()  # Run optimization\n",
    "    X, fX = turbo_m.X, turbo_m.fX  # Evaluated points\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        f,\n",
    "        lb,\n",
    "        ub,\n",
    "        n_init,\n",
    "        max_evals,\n",
    "        n_trust_regions,\n",
    "        batch_size=1,\n",
    "        verbose=True,\n",
    "        use_ard=True,\n",
    "        max_cholesky_size=2000,\n",
    "        n_training_steps=50,\n",
    "        min_cuda=1024,\n",
    "        device=\"cpu\",\n",
    "        dtype=\"float64\",\n",
    "        utility_function_class=None,\n",
    "    ):\n",
    "        self.n_trust_regions = n_trust_regions\n",
    "        super().__init__(\n",
    "            f=f,\n",
    "            lb=lb,\n",
    "            ub=ub,\n",
    "            n_init=n_init,\n",
    "            max_evals=max_evals,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            use_ard=use_ard,\n",
    "            max_cholesky_size=max_cholesky_size,\n",
    "            n_training_steps=n_training_steps,\n",
    "            min_cuda=min_cuda,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "        self.succtol = 3\n",
    "        self.failtol = max(5, self.dim)\n",
    "        self.utility_function_class = utility_function_class or ExpectedImprovementCustom\n",
    "\n",
    "        # Very basic input checks\n",
    "        assert n_trust_regions > 1 and isinstance(max_evals, int)\n",
    "        assert max_evals > n_trust_regions * n_init, \"Not enough evaluations for initial points\"\n",
    "        assert max_evals > batch_size, \"Not enough evaluations to do a single batch\"\n",
    "\n",
    "        # Remember the hypers for trust regions we don't sample from\n",
    "        self.hypers = [{} for _ in range(self.n_trust_regions)]\n",
    "\n",
    "        # Initialize parameters\n",
    "        self._restart()\n",
    "\n",
    "    def _restart(self):\n",
    "        self._idx = np.zeros((0, 1), dtype=int)  # Track which trust region proposed each point\n",
    "        self.failcount = np.zeros(self.n_trust_regions, dtype=int)\n",
    "        self.succcount = np.zeros(self.n_trust_regions, dtype=int)\n",
    "        self.length = self.length_init * np.ones(self.n_trust_regions)\n",
    "\n",
    "    def _adjust_length(self, fX_next, i):\n",
    "        assert 0 <= i <= self.n_trust_regions - 1\n",
    "\n",
    "        fX_min = self.fX[self._idx[:, 0] == i, 0].min()  # Best value in trust region i\n",
    "        if fX_next.min() < fX_min - 1e-3 * math.fabs(fX_min):\n",
    "            self.succcount[i] += 1\n",
    "            self.failcount[i] = 0\n",
    "        else:\n",
    "            self.succcount[i] = 0\n",
    "            self.failcount[i] += len(fX_next)  # Add batch size for this trust region\n",
    "\n",
    "        if self.succcount[i] == self.succtol:\n",
    "            self.length[i] = min([2.0 * self.length[i], self.length_max])\n",
    "            self.succcount[i] = 0\n",
    "        elif self.failcount[i] >= self.failtol:\n",
    "            self.length[i] /= 2.0\n",
    "            self.failcount[i] = 0\n",
    "\n",
    "    def _select_candidates(self, X_cand, y_cand):\n",
    "        \"\"\"Select candidates from samples from all trust regions.\"\"\"\n",
    "        assert X_cand.shape == (self.n_trust_regions, self.n_cand, self.dim)\n",
    "        assert y_cand.shape == (self.n_trust_regions, self.n_cand)\n",
    "        assert X_cand.min() >= 0.0 and X_cand.max() <= 1.0 and np.all(np.isfinite(y_cand))\n",
    "\n",
    "        X_next = np.zeros((self.batch_size, self.dim))\n",
    "        idx_next = np.zeros((self.batch_size, 1), dtype=int)\n",
    "        for k in range(self.batch_size):\n",
    "            i, j = np.unravel_index(np.argmin(y_cand), y_cand.shape)\n",
    "            X_next[k, :] = deepcopy(X_cand[i, j, :])\n",
    "            idx_next[k, 0] = i\n",
    "            y_cand[i, j] = np.inf  # Exclude this point from future selection\n",
    "\n",
    "        return X_next, idx_next\n",
    "\n",
    "    def _create_candidates(self, X, fX, length, n_training_steps, hypers):\n",
    "    # Convert data to torch tensors\n",
    "        X_torch = torch.tensor(X, dtype=self.dtype, device=self.device)\n",
    "        fX_torch = torch.tensor(fX, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        # Normalize the outputs\n",
    "        y_mean = fX_torch.mean()\n",
    "        y_std = fX_torch.std()\n",
    "        fX_normalized = (fX_torch - y_mean) / y_std\n",
    "\n",
    "        # Train GP model using your train_gp function\n",
    "        if hypers is not None and hypers != {}:\n",
    "            # If hypers are provided, load them and set num_steps to 0\n",
    "            model = train_gp(\n",
    "                train_x=X_torch,\n",
    "                train_y=fX_normalized,\n",
    "                use_ard=self.use_ard,\n",
    "                num_steps=0,\n",
    "                hypers=hypers,\n",
    "            )\n",
    "        else:\n",
    "            # Train the model and get new hypers\n",
    "            model = train_gp(\n",
    "                train_x=X_torch,\n",
    "                train_y=fX_normalized,\n",
    "                use_ard=self.use_ard,\n",
    "                num_steps=n_training_steps,\n",
    "            )\n",
    "            # Save the model's state_dict as hypers for future use\n",
    "            hypers = model.state_dict()\n",
    "\n",
    "        # Define the acquisition function using your utility function class\n",
    "        best_f = fX_normalized.min().item()\n",
    "        acquisition_function = self.utility_function_class(model, best_f=best_f)\n",
    "\n",
    "        # Optimize the acquisition function to find candidate points\n",
    "        X_cand = self.optimize_acquisition_function(acquisition_function, length)\n",
    "        X_cand = X_cand.detach().cpu().numpy()\n",
    "\n",
    "        # Evaluate the acquisition function at candidate points\n",
    "        y_cand = acquisition_function(\n",
    "            torch.tensor(X_cand, dtype=self.dtype, device=self.device)\n",
    "        )\n",
    "        y_cand = y_cand.detach().cpu().numpy().ravel()\n",
    "\n",
    "        return X_cand, y_cand, hypers\n",
    "\n",
    "    def optimize_acquisition_function(self, acquisition_function, length):\n",
    "        # Ensure self.center is a torch tensor\n",
    "        if not isinstance(self.center, torch.Tensor):\n",
    "            self.center = torch.tensor(self.center, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        # Define bounds for the trust region using torch.clamp\n",
    "        tr_lb = torch.clamp(self.center - length / 2.0, min=0.0, max=1.0)\n",
    "        tr_ub = torch.clamp(self.center + length / 2.0, min=0.0, max=1.0)\n",
    "        \n",
    "        # Stack bounds into a tensor\n",
    "        bounds = torch.stack([tr_lb, tr_ub])\n",
    "\n",
    "        # Generate initial points within the bounds\n",
    "        X_init = latin_hypercube(self.n_cand, self.dim)\n",
    "        X_init = torch.tensor(X_init, dtype=self.dtype, device=self.device)\n",
    "        X_init = tr_lb + (tr_ub - tr_lb) * X_init  # Scale to trust region bounds\n",
    "\n",
    "        # Optimize the acquisition function\n",
    "        X_cand, _ = optimize_acqf(\n",
    "            acq_function=acquisition_function,\n",
    "            bounds=bounds,\n",
    "            q=self.batch_size,\n",
    "            num_restarts=10,\n",
    "            raw_samples=100,\n",
    "            options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        )\n",
    "        return X_cand\n",
    "\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"Run the full optimization process.\"\"\"\n",
    "        # Create initial points for each trust region\n",
    "        for i in range(self.n_trust_regions):\n",
    "            X_init = latin_hypercube(self.n_init, self.dim)\n",
    "            X_init = from_unit_cube(X_init, self.lb, self.ub)\n",
    "            fX_init = np.array([[self.f(x)] for x in X_init])\n",
    "\n",
    "            # Update budget and set as initial data for this trust region\n",
    "            self.X = np.vstack((self.X, X_init))\n",
    "            self.fX = np.vstack((self.fX, fX_init))\n",
    "            self._idx = np.vstack((self._idx, i * np.ones((self.n_init, 1), dtype=int)))\n",
    "            self.n_evals += self.n_init\n",
    "\n",
    "            if self.verbose:\n",
    "                fbest = fX_init.min()\n",
    "                print(f\"TR-{i} starting from: {fbest:.4f}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        # Main optimization loop\n",
    "        while self.n_evals < self.max_evals:\n",
    "\n",
    "            # Generate candidates from each trust region\n",
    "            X_cand = np.zeros((self.n_trust_regions, self.n_cand, self.dim))\n",
    "            y_cand = np.inf * np.ones((self.n_trust_regions, self.n_cand))\n",
    "            for i in range(self.n_trust_regions):\n",
    "                idx = np.where(self._idx == i)[0]  # Indices of points in trust region i\n",
    "\n",
    "                # Get the points and values for trust region i\n",
    "                X = deepcopy(self.X[idx, :])\n",
    "                X = to_unit_cube(X, self.lb, self.ub)\n",
    "                fX = deepcopy(self.fX[idx, 0].ravel())\n",
    "\n",
    "                # Determine if model retraining is needed\n",
    "                n_training_steps = 0 if self.hypers[i] else self.n_training_steps\n",
    "\n",
    "                # Create new candidates\n",
    "                self.center = X[fX.argmin(), :]  # Update center to best point in TR\n",
    "                X_cand[i, :, :], y_cand[i, :], self.hypers[i] = self._create_candidates(\n",
    "                    X,\n",
    "                    fX,\n",
    "                    length=self.length[i],\n",
    "                    n_training_steps=n_training_steps,\n",
    "                    hypers=self.hypers[i],\n",
    "                )\n",
    "\n",
    "            # Select the next candidates\n",
    "            X_next, idx_next = self._select_candidates(X_cand, y_cand)\n",
    "            assert X_next.min() >= 0.0 and X_next.max() <= 1.0\n",
    "\n",
    "            # Undo the warping\n",
    "            X_next = from_unit_cube(X_next, self.lb, self.ub)\n",
    "\n",
    "            # Evaluate batch\n",
    "            fX_next = np.array([[self.f(x)] for x in X_next])\n",
    "\n",
    "            # Update trust regions\n",
    "            for i in range(self.n_trust_regions):\n",
    "                idx_i = np.where(idx_next == i)[0]\n",
    "                if len(idx_i) > 0:\n",
    "                    self.hypers[i] = {}  # Remove model hypers\n",
    "                    fX_i = fX_next[idx_i]\n",
    "\n",
    "                    if self.verbose and fX_i.min() < self.fX.min() - 1e-3 * abs(self.fX.min()):\n",
    "                        n_evals, fbest = self.n_evals, fX_i.min()\n",
    "                        print(f\"{n_evals}) New best @ TR-{i}: {fbest:.4f}\")\n",
    "                        sys.stdout.flush()\n",
    "                    self._adjust_length(fX_i, i)\n",
    "\n",
    "            # Update budget and append data\n",
    "            self.n_evals += self.batch_size\n",
    "            self.X = np.vstack((self.X, deepcopy(X_next)))\n",
    "            self.fX = np.vstack((self.fX, deepcopy(fX_next)))\n",
    "            self._idx = np.vstack((self._idx, deepcopy(idx_next)))\n",
    "\n",
    "            # Check if any trust region needs to be restarted\n",
    "            for i in range(self.n_trust_regions):\n",
    "                if self.length[i] < self.length_min:\n",
    "                    idx_i = self._idx[:, 0] == i\n",
    "\n",
    "                    if self.verbose:\n",
    "                        n_evals, fbest = self.n_evals, self.fX[idx_i, 0].min()\n",
    "                        print(f\"{n_evals}) TR-{i} converged to: {fbest:.4f}\")\n",
    "                        sys.stdout.flush()\n",
    "\n",
    "                    # Reset length and counters, remove old data from trust region\n",
    "                    self.length[i] = self.length_init\n",
    "                    self.succcount[i] = 0\n",
    "                    self.failcount[i] = 0\n",
    "                    self._idx[idx_i, 0] = -1  # Remove points from trust region\n",
    "                    self.hypers[i] = {}  # Remove model hypers\n",
    "\n",
    "                    # Create a new initial design\n",
    "                    X_init = latin_hypercube(self.n_init, self.dim)\n",
    "                    X_init = from_unit_cube(X_init, self.lb, self.ub)\n",
    "                    fX_init = np.array([[self.f(x)] for x in X_init])\n",
    "\n",
    "                    # Print progress\n",
    "                    if self.verbose:\n",
    "                        n_evals, fbest = self.n_evals, fX_init.min()\n",
    "                        print(f\"{n_evals}) TR-{i} is restarting from: {fbest:.4f}\")\n",
    "                        sys.stdout.flush()\n",
    "\n",
    "                    # Append data to local history\n",
    "                    self.X = np.vstack((self.X, X_init))\n",
    "                    self.fX = np.vstack((self.fX, fX_init))\n",
    "                    self._idx = np.vstack((self._idx, i * np.ones((self.n_init, 1), dtype=int)))\n",
    "                    self.n_evals += self.n_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dtype = torch.float64 \n",
      "Using device = cpu\n",
      "TR-0 starting from: 1.1418\n",
      "TR-1 starting from: 0.6647\n",
      "TR-2 starting from: 0.7114\n",
      "TR-3 starting from: 1.0791\n",
      "TR-4 starting from: 1.1284\n",
      "50) New best @ TR-1: 0.4700\n",
      "53) New best @ TR-1: 0.1656\n",
      "54) New best @ TR-1: 0.0988\n",
      "56) New best @ TR-1: 0.0513\n",
      "57) New best @ TR-1: 0.0068\n",
      "61) New best @ TR-1: 0.0000\n",
      "97) TR-1 converged to: 0.0000\n",
      "97) TR-1 is restarting from: 0.2978\n",
      "155) TR-3 converged to: 0.0000\n",
      "155) TR-3 is restarting from: 0.7751\n",
      "Best value found: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from turboturbo_m import TurboM  # Ensure TurboM is imported correctly\n",
    "from utility import ExpectedImprovementCustom  # Import your utility function class\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def sphere_function(x):\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "# Problem dimensions and bounds\n",
    "dim = 5\n",
    "lb = np.zeros(dim)\n",
    "ub = np.ones(dim)\n",
    "\n",
    "# Initialize the TuRBO-M optimizer\n",
    "turbo_m = TurboM(\n",
    "    f=sphere_function,\n",
    "    lb=lb,\n",
    "    ub=ub,\n",
    "    n_init=2 * dim,\n",
    "    max_evals=200,\n",
    "    n_trust_regions=5,\n",
    "    batch_size=1,\n",
    "    verbose=True,\n",
    "    utility_function_class=ExpectedImprovementCustom,\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "turbo_m.optimize()\n",
    "\n",
    "# Retrieve the evaluated points and corresponding function values\n",
    "X, fX = turbo_m.X, turbo_m.fX\n",
    "\n",
    "# Print the best found value\n",
    "print(f\"Best value found: {fX.min():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
